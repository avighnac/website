<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ai-asr â€” Blog</title>
  <link rel="stylesheet" href="../assets/styles.css" />
  <link rel="icon" type="image/svg+xml" href="../images/favicon/favicon.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="../images/favicon/favicon_32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../images/favicon/favicon_16x16.png">
  <link rel="icon" type="image/png" sizes="48x48" href="../images/favicon/favicon_48x48.png">
  <link rel="icon" type="image/png" sizes="64x64" href="../images/favicon/favicon_64x64.png">
  <link rel="icon" type="image/png" sizes="128x128" href="../images/favicon/favicon_128x128.png">
  <link rel="shortcut icon" href="../images/favicon/favicon.ico">
</head>

<body>
  <div id="header-placeholder">
    <header>
      <nav>
        <a href="/">Home</a>
        <a href="/about">About</a>
        <a href="/projects">Projects</a>
        <a href="/achievements">Achievements</a>
        <a href="/resume">RÃ©sumÃ©</a>
        <a href="/blog">Blog</a>
      </nav>
    </header>
  </div>

  <main class="blog-page">
    <article class="blog-post">
      <h1>ğŸ—£ï¸ Why Canâ€™t My AI Just Write Down the Words I Screamed Into My Mic at 3AM</h1>

      <p>
        Quick question: why havenâ€™t people been able to make good automatic-speech-recognition (ASR) AI models yet?
      </p>

      <p>
        I mean, weâ€™ve done everything else. Image generation? Done. Video synthesis? Yep. o3 has a Codeforces rating of
        2500 and an IOI silver. o3-IOI (under relaxed constraints) has IOI gold. Literal language parsing? Solved. Even
        TTS is basically perfectâ€”natural-sounding, emotive, intonation-aware.
      </p>

      <p>
        But ASR? It still flops.
      </p>

      <p>
        I mean, be honestâ€”when was the last time you actually used your phoneâ€™s voice transcription feature
        unironically? You havenâ€™t. No one has. Youâ€™ve seen the icon a hundred times, but your brain just skips over it.
      </p>

      <p>
        ChatGPTâ€™s voice transcription is goodâ€”very good, in factâ€”but youâ€™ll still find one horribly misinterpreted
        sentence every 3â€“4 lines. Itâ€™s amazing at punctuation. Itâ€™s almost there. But that one failure always reminds
        you: this model doesnâ€™t really understand what youâ€™re saying.
      </p>

      <p>So whatâ€™s going on? Is ASR just that much harder than everything else?</p>

      <p class="blog-transition">Letâ€™s try some explanations.</p>

      <p>Maybe speech is fundamentally ambiguous in a way other modalities arenâ€™t. Sure. Humans speak in noisy, mumbled,
        overlapping, dialect-specific ways. Sounds like a decent explanation.</p>

      <p>â€¦ but wait. No. That shouldnâ€™t be such a big deal. TTSâ€” text-to-speechâ€” works just fine. And it deals with a
        lot
        of the same challenges. Images can be noisy. So can handwriting. In fact, not only is handwriting
        person-specific, itâ€™s often borderline illegibleâ€” and yet OCR works incredibly well.</p>

      <p>Even language itself is messyâ€” English especially. But LLMs figured it out. This isnâ€™t enough.</p>

      <p class="blog-transition">Okay. Letâ€™s try again.</p>

      <p>
        Maybe the issue is that audio data is noisy, varied, and contextual. It includes emotional tone, sarcasm,
        interruptions, echo, background noise. Long-form dependencies. No word boundaries. No punctuation.
      </p>

      <p>
        Sure, this is closer to something meaningful. But Iâ€™m still not convinced.
      </p>

      <ul>
        <li>â€œNoise and variationâ€? Already talked about that. Not unique.</li>
        <li>â€œLong contextâ€? Gemini has a context length of a million tokens. And besides, weâ€™ve made huge strides in
          summarization, code tracing, and story coherence.</li>
        <li>â€œOh, but tokens donâ€™t exist in audio.â€ Okayâ€”fair. But OCR doesnâ€™t have token boundaries either. And models
          handle that surprisingly well.</li>
      </ul>

      <p>So again: why is speech the thing that breaks AI?</p>

      <p>Letâ€™s eliminate what speech isnâ€™t uniquely worse at: noisy input, ambiguous structure, and a lack of clear
        tokens: all three things already demonstrated by images, language, and handwritten text.</p>

      <p>None of these explain the gap. So what is different?</p>

      <p class="blog-transition">Hereâ€™s the actual problem: Human speech is intrinsically temporal and compositional in
        a way no other AI input is. Speech is not just input. It evolves in time, and interpreting it requires joint
        reasoning over timing, memory,
        semantics, and expectation.</p>

      <p>When you listen to someone say:</p>

      <blockquote>â€œShe said the man whoâ€¦ uhâ€¦ the, umâ€¦ doctorâ€”not the other oneâ€”â€</blockquote>

      <p>You track intent, conversational goal, and whoâ€™s being corrected. Sense social dynamics. Use prosody
        (intonation), pauses, repetition, disfluenciesâ€”all of which affect meaning.</p>

      <p>How do we encode these into parameters? LLMs never had to learn that off the bat. Even OCR never had to do
        something this complex. Vision models? Not even close. And even Whisper doesnâ€™t.</p>

      <p class="blog-transition">Another issue: thereâ€™s no standardized symbolic representation of sound.</p>

      <div class="symbol-map">
        <div>
          <strong>Text</strong>
          <svg viewBox="0 0 60 16" class="inline-arrow">
            <line x1="0" y1="8" x2="55" y2="8" />
            <polyline points="50,4 55,8 50,12" />
          </svg>
          tokens
        </div>
        <div>
          <strong>Images</strong>
          <svg viewBox="0 0 60 16" class="inline-arrow">
            <line x1="0" y1="8" x2="55" y2="8" />
            <polyline points="50,4 55,8 50,12" />
          </svg>
          pixels
        </div>
        <div>
          <strong>OCR</strong>
          <svg viewBox="0 0 60 16" class="inline-arrow">
            <line x1="0" y1="8" x2="55" y2="8" />
            <polyline points="50,4 55,8 50,12" />
          </svg>
          glyphs
        </div>
        <div>
          <strong>Code</strong>
          <svg viewBox="0 0 60 16" class="inline-arrow tooltip-arrow">
            <line x1="0" y1="8" x2="55" y2="8" />
            <polyline points="50,4 55,8 50,12" />
          </svg>
          <span class="hover-detail"
            data-tooltip="This oneâ€™s so clean it barely countsâ€”we literally have compilers.">ASTs</span>
        </div>
      </div>

      <p>But what about audio? Can we just use the frequencies of the raw waveform? Nope. Because symbolic
        representation isnâ€™t just raw dataâ€”it has to carry extractable meaning in a way thatâ€™s both structured and
        learnable. Thatâ€™s why we tokenize text instead of feeding in raw characters.</p>

      <p>And even if we did manage to impose some structured representation on the raw waveformâ€”something that feels
        vaguely symbolicâ€”what would we actually map it to?</p>

      <ul>
        <li>Phonemes? Language-dependent, fuzzy, no clean boundaries</li>
        <li>Syllables? Too coarse</li>
        <li>Audio frames? Arbitrary sampling choice, meaningless without structure</li>
      </ul>

      <p class="blog-transition">Humans, meanwhile, solve this via specialized hardware (our cochlea), pre-baked
        representation
        compression, and years of exposure during a critical period. </p>

      <p>
        Oh, and one more thing: human speech comprehension is massively top-down<span class="interruption">â€”</span>
      </p>

      <p class="blog-hard-break">Wait a minute.</p>

      <p>
        That partâ€”â€œhumans solve this via specialized hardwareâ€ â€¦ that actually sounds familiar. Doesnâ€™t this describe
        every single machine learning system ever? The â€œspecialized hardwareâ€ is just the protocolâ€”the matrix
        multiplications and architectural structure baked into the GPU. The â€œpre-baked representationâ€ becomes tuned
        weights and dense learned embeddings. And â€œyears of exposureâ€? Thatâ€™s just training epochs, stretched out over
        gradient descent.
      </p>

      <p>So â€¦ whatâ€™s the difference?</p>

      <p class="blog-transition">Ah. Hereâ€™s where the illusion of equivalence breaks down.</p>

      <p>Yes, it looks like a clean mapping:</p>

      <div class="symbol-map">
        <div>
          <strong>Cochlea</strong>
          <svg viewBox="0 0 60 16" class="inline-arrow">
            <line x1="0" y1="8" x2="55" y2="8" />
            <polyline points="50,4 55,8 50,12" />
          </svg>
          Neural net architecture
        </div>
        <div>
          <strong>Representation</strong>
          <svg viewBox="0 0 60 16" class="inline-arrow">
            <line x1="0" y1="8" x2="55" y2="8" />
            <polyline points="50,4 55,8 50,12" />
          </svg>
          Learned embeddings, layers
        </div>
        <div>
          <strong>Years of exposure</strong>
          <svg viewBox="0 0 60 16" class="inline-arrow">
            <line x1="0" y1="8" x2="55" y2="8" />
            <polyline points="50,4 55,8 50,12" />
          </svg>
          Training epochs, big data
        </div>
      </div>

      <p>But these arenâ€™t equivalent in structure. The human auditory system isnâ€™t just a fancy feature extractor. Itâ€™s
        a dynamically wired preprocessor shaped by millions of years of evolutionary pressure.</p>

      <p>When we say â€œcochlea,â€ weâ€™re not just referring to a 1D convolutional kernel. The biological cochlea performs
        nonlinear, frequency-specific compression, automatically mapping frequencies to perceived pitch on a logarithmic
        scale. It adapts in real time to filter noise, and instead of outputting dense float vectors, it transmits
        spike-based signals through parallel neural pathways. Most critically, it interfaces directly with attention
        systemsâ€”letting us focus on a single voice in a noisy room.
      </p>

      <p>With near-zero latency. This isn't just some â€˜cleverâ€™ encoder: it's more like an inductive bias, hardwired into
        biology, evolved over 500 million years.</p>

      <p class="blog-transition">Todayâ€™s ASR models? They typically take in raw waveforms (yep, that idea we had
        earlier) or Mel spectrograms. Then they run it through batch norm, convolutional layers, and transformers. But
        hereâ€™s the problem: nothing in that process guarantees that phase, rhythm, stress, or even the overall auditory
        scene structure will be preserved.</p>

      <p>You could say the function is similar. But the alignment is offâ€”by layers and layers of abstraction.</p>

      <p>And by the wayâ€”backpropagation is not neuroplasticity. Why? Backprop works by computing global gradients across
        many layers and adjusting weights. The brain doesnâ€™t do that. It relies on local, spike-timing-dependent
        plasticityâ€”more Hebbian than SGD. Thereâ€™s no global loss function to minimize in your head. In fact, your brain
        runs on dozens of local, competing, self-regulating modules.</p>

      <p>AI learns exactly what itâ€™s told to optimize. Humans learn whatâ€™s meaningful, adaptive, and predictiveâ€”whether
        or not thereâ€™s supervision. And this becomes super important when your â€œtraining dataâ€ isnâ€™t as clean as:</p>

      <blockquote>â€œThis audio clip says exactly this.â€</blockquote>

      <p>Add contextual bias to the mix, and the cracks really show. Thatâ€™s why an ASR model can hear:</p>

      <blockquote>â€œThe politician resigned amid controversyâ€</blockquote>

      <p>and confidently transcribe:</p>

      <blockquote>â€œThe politician re-signed amid controversyâ€</blockquote>

      <p>Same audio. Same phonemes.
        But to a human? That obviously makes no sense in context. Itâ€™s not a common collocation, and it breaks the
        expected narrative logic. Youâ€™d correct it automatically, unconsciously.</p>

      <p class="blog-transition">So whatâ€™s the actual problem again? Right. Systems arenâ€™t linked.</p>

      <p>That becomes the core insight: for AI models to get better, they have to start behaving more like brains. Maybe
        that means building in competition between modules. Maybe it means unsupervised learning, or predictive
        correction loops. Maybe it means integrating multiple subsystems with partially shared context.</p>

      <p>Who knowsâ€”maybe thatâ€™s the missing ingredient. Imagine mimicking the cochleaâ€™s nonlinear, frequency-specific
        compression. Pass in audio by frequency instead of just time-sliced waveforms.</p>

      <p>Imagine interfacing with other AI systems that think about which syllables are actually relevant. Let them
        compete. Let them argue. Maybe introduce some kind of evolutionary learning pressure. Maybe bring in social
        cues.</p>

      <p class="blog-transition">Actually, come to think of itâ€”weâ€™ve already started down this path. Weâ€™ve mimicked the
        retina in convolutional
        layers, the hippocampus in episodic memory modules, and attention in transformer architectures.</p>

      <p>But we havenâ€™t even touched the full auditory pathway, self-supervised predictive sensorimotor integration,
        competitive modules that argue internally, or curiosity, clarification-seeking, and error awareness (!!).
        Implementing these wouldn't just make Whisper++â€” you'd get something that looks like a nervous system.</p>

      <p class="blog-transition">And yeahâ€”if you said in 2014,</p>
      <blockquote>â€œA multilingual model will understand 100+ languages.â€</blockquote>
      <p>No one would believe you.</p>

      <p>2016:</p>
      <blockquote>â€œA text generator that passes law exams.â€</blockquote>
      <p>No one would believe you, and theyâ€™d probably think youâ€™re ridiculous.</p>

      <p>2024:</p>
      <blockquote>â€œA model playing Codeforces at (grand)master level.â€</blockquote>
      <p>No one would believe you, theyâ€™d think youâ€™re ridiculous, and theyâ€™d probably go on a 7-paragraph thread about
        how you â€œdonâ€™t understand the deep beauty of competitive programmingâ€ and how â€œLLMs will never replicate the
        human spark behind a CF 2800.â€</p>

      <p>But they all happened. Because every time we think a problem is too hard, we realize we just didnâ€™t understand
        the structure of the solution yet.</p>

      <p class="blog-transition">And hereâ€™s the part that gives me chills: weâ€™re going full circle.</p>

      <p>
        Have you noticed how every meaningful AI breakthrough strongly mimics something biological? Not just loosely
        inspiredâ€”but architecturally convergent. Episodic memory shows up in neural networks as memory modules. Sparse
        activation mirrors how real neurons only fire occasionally. Spiking neurons are now being explored even in
        neuromorphic hardware. And multimodal integration? That's straight from how our brains process vision, sound,
        and language together.
      </p>

      <p>We keep rediscovering what the brain already knew. Because we have no other choice. Every time we advance AI in
        a meaningful way, we end up rediscovering biology. There may be only a handful of workable architectures for
        general intelligence, and evolution already found one
        of them.</p>

      <p class="blog-transition">So what does this tell us about AI?</p>

      <p>It suggests that intelligence isnâ€™t arbitrary. It's emergent structure, rather than something that can be
        brute-forced by adding layers, tokens, or data, shaped by a set of computation that evolution discovered under
        the constraints of:</p>
      <ul>
        <li>Energy efficiency</li>
        <li>Adaptability</li>
        <li>Fault tolerance</li>
        <li>Long-term learning</li>
        <li>Generalization from limited data</li>
      </ul>

      <p>So when we finally build AI systems that reason like us, remember like us, and even mishear like usâ€”donâ€™t be
        surprised if it ends up mimicking biology way more than you expected. Thatâ€™s not because weâ€™re trying to be
        poetic. Itâ€™s because biology already solved the optimization problem weâ€™re now chasing.</p>

      <p class="blog-transition">What does this tell us about humans?</p>

      <p>That we're not separate from intelligence, or its peakâ€” we're just one instance of a general class of
        intelligent systems. What we call 'human thought' is just one configuration of a substrate shaped to deal with a
        messy, dynamic, uncertain world.</p>

      <p>The more we build these systems, the more it feels like weâ€™re just reverse-engineering ourselves. And the
        solution starts to look eerily familiar â€¦ because maybe, under these rules, it's the only one that works.</p>

      <p>Which also means: Biology isnâ€™t a limitationâ€” itâ€™s a map that we've only now started decoding.</p>

      <p class="blog-transition">
        And the broader picture? We are, in a way, witnessing a recursion of minds: nature built a brain. The brain
        built machines. And now, the
        machines have started reconstructing the brain.
      </p>

      <p>
        (And who knowsâ€”maybe weâ€™re one of the brains some earlier machine reconstructed. Thatâ€™s a fun discussion for the
        <a href="https://en.wikipedia.org/wiki/Simulation_hypothesis" target="_blank"
          rel="noopener noreferrer">simulation theory</a> blog. Not this one.)
      </p>

      <p class="blog-transition">Wait, what was the topic again?</p>

      <p>Oh right. Automatic-speech-recognition. But youâ€™ve probably realized by now: this wasnâ€™t really a blog about
        ASR, was it?</p>

      <p>That was just the entry point: The bait. The 3AM voicemail that dragged you into cognition, systems
        neuroscience, and recursive structure.</p>

      <p>So thenâ€”letâ€™s answer the original question:</p>
      <blockquote>Why canâ€™t my AI just write down the words I screamed into my mic at 3AM?</blockquote>

      <p>Because understanding isnâ€™t transcription. Itâ€™s not audio decoding, or phoneme recognition, or even language
        modeling.</p>

      <p>Understanding is the whole thing.</p>

    </article>
  </main>

  <div id="footer-placeholder">
    <footer>
      <div class="footer-container">
        <p>&copy; 2025 Avighna Chhatrapati</p>
        <p>
          <a href="https://linkedin.com/in/avighnakc" target="_blank" rel="noopener noreferrer">LinkedIn</a> &middot;
          <a href="mailto:avighnakc@gmail.com">Email</a> &middot;
          <a href="https://github.com/avighnac" target="_blank" rel="noopener noreferrer">GitHub</a>
        </p>
      </div>
    </footer>
  </div>
  <script src="../assets/script.js"></script>
</body>

</html>